{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://images.efollett.com/htmlroot/images/templates/storeLogos/CA/864.gif\" style=\"float: right;\"> \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ECON628-01 \n",
    "### Lecture 1.1 - NLP\n",
    "---\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Language?\n",
    "---\n",
    "![](https://snag.gy/BXQ6xM.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is NLP?\n",
    "---\n",
    "**NLP =** Natural Language Processing => Computational Linguistic, Text Analysis, etc.\n",
    "\n",
    "- Using computers to process (analyze, understand, generate) natural human languages \n",
    "- Most knowledge created by humans is _unstructured text_, and we need a way to make sense of it\n",
    "- We need to reach to a quantitative analysis of text\n",
    "- Also referred to as machine learning with text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### NLP Applications?\n",
    "---\n",
    "\n",
    "- **Information retrieval**: Find relevant results and similar results\n",
    "    - [Google](https://www.google.com/)\n",
    "- **Information extraction**: Structured information from unstructured documents\n",
    "    - [Events from Gmail](https://support.google.com/calendar/answer/6084018?hl=en)\n",
    "- **Machine translation**: One language to another\n",
    "    - [Google Translate](https://translate.google.com/)\n",
    "- **Text simplification**: Preserve the meaning of text, but simplify the grammar and vocabulary\n",
    "    - [Rewordify](https://rewordify.com/)\n",
    "    - [Simple English Wikipedia](https://simple.wikipedia.org/wiki/Main_Page)\n",
    "- **Predictive text input**: Faster or easier typing\n",
    "    - [A friend's application](https://justmarkham.shinyapps.io/textprediction/)\n",
    "    - [A much better application](https://farsite.shinyapps.io/swiftkey-cap/)\n",
    "- **Sentiment analysis**: Attitude of speaker\n",
    "    - [Hater News](http://haternews.herokuapp.com/)\n",
    "- **Automatic summarization**: Extractive or abstractive summarization\n",
    "    - [autotldr](https://www.reddit.com/r/technology/comments/35brc8/21_million_people_still_use_aol_dialup/cr2zzj0)\n",
    "- **Natural Language Generation**: Generate text from data\n",
    "    - [How a computer describes a sports match](http://www.bbc.com/news/technology-34204052)\n",
    "    - [Publishers withdraw more than 120 gibberish papers](http://www.nature.com/news/publishers-withdraw-more-than-120-gibberish-papers-1.14763)\n",
    "- **Speech recognition and generation**: Speech-to-text, text-to-speech\n",
    "    - [Google's Web Speech API demo](https://www.google.com/intl/en/chrome/demos/speech.html)\n",
    "    - [Vocalware Text-to-Speech demo](https://www.vocalware.com/index/demo)\n",
    "- **Question answering**: Determine the intent of the question, match query with knowledge base, evaluate hypotheses\n",
    "    - [How did supercomputer Watson beat Jeopardy champion Ken Jennings?](http://blog.ted.com/how-did-supercomputer-watson-beat-jeopardy-champion-ken-jennings-experts-discuss/)\n",
    "    - [IBM's Watson Trivia Challenge](http://www.nytimes.com/interactive/2010/06/16/magazine/watson-trivia-game.html)\n",
    "    - [The AI Behind Watson](http://www.aaai.org/Magazine/Watson/watson.php)\n",
    "- **Spell and Grammar Checking**: Checking spelling and grammar & suggesting alternatives for the errors\n",
    "    - [Grammarly](https://app.grammarly.com)\n",
    "- **Word Prediction**: Search for something in Google\n",
    "    - [Google](https://www.google.com/)\n",
    "- **Text Categorization**: Assigning one (or more) pre-defined category to a text\n",
    "    - [PubMed](https://www.ncbi.nlm.nih.gov/pubmed/)  \n",
    "- **Sentiment Analysis**: Reviews (amazon => customers reviews, imbdr => movies reviews, etc)\n",
    "    - [Amazon](https://www.amazon.com/)    \n",
    "- **Speech Recognition**: Siri, Alexa\n",
    "    - [Siri](https://www.apple.com/ios/siri/) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### NLP general applications -  level of difficulty\n",
    "---\n",
    "- **Easy (mostly solved)**:\n",
    "    - Spell and grammar checking\n",
    "    - Some text categorization tasks\n",
    "    - Some named-entity recognition tasks\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "- **Intermediate (good progress)**:\n",
    "    - Information retrieval\n",
    "    - Sentiment analysis\n",
    "    - Machine translation\n",
    "    - Information extraction\n",
    "\n",
    "&nbsp;    \n",
    "\n",
    "- **Difficult (still hard)**:\n",
    "    - Question answering (Match query with knowledge base)\n",
    "    - Summarization\n",
    "    - Speech Recognition;\n",
    "        - Speech to text\n",
    "        - Trained/untrained user models\n",
    "        - Voice-based interfaces\n",
    "    - Closed domain vs open domain\n",
    "    - Reasoning about intent of question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### NLP basic components\n",
    "---\n",
    "- **Tokenization**: breaking text into tokens (words, sentences, n-grams)\n",
    "- **Stopword removal**: a/an/the\n",
    "- **Stemming and lemmatization**: root word\n",
    "- **TF-IDF**: word importance\n",
    "- **Part-of-speech tagging**: noun/verb/adjective\n",
    "- **Named entity recognition**: person/organization/location\n",
    "- **Spelling correction**: \"New Yrok City\"\n",
    "- **Word sense disambiguation**: \"buy a mouse\"\n",
    "- **Segmentation**: \"New York City subway\"\n",
    "- **Language detection**: \"translate this page\"\n",
    "- **Machine learning**: (classification, clustering, recommendation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why is NLP hard?\n",
    "----\n",
    "\n",
    "- **Ambiguity**:\n",
    "    - Hospitals are Sued by 7 Foot Doctors\n",
    "    - Juvenile Court to Try Shooting Defendant\n",
    "    - Local High School Dropouts Cut in Half\n",
    "- **Non-standard English**: text messages\n",
    "- **Idioms**: \"throw in the towel\"\n",
    "- **Newly coined words**: \"retweet\"\n",
    "- **Tricky entity names**: \"Where is A Bug's Life playing?\"\n",
    "- **World knowledge**: \"Mary and Sue are sisters\", \"Mary and Sue are mothers\"\n",
    "- **Texts with the same words and phrases can having different meanings **: \n",
    "State farm commercial where two different people say \"Is this my car? What? This is ridiculous! This can't be happening! Shut up! Ahhhh!!!\"\n",
    "\n",
    "\n",
    "NLP requires an understanding of the **language** and the **world**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Common NLP Related Models\n",
    "---\n",
    "\n",
    "- **LSI**: Latent semantic indexing\n",
    "    - [Latent semantic indexing](https://nlp.stanford.edu/IR-book/html/htmledition/latent-semantic-indexing-1.html) \n",
    "- **LDA**: Latent Dirichlet Allocation\n",
    "    - [Latent Dirichlet Allocation](https://algorithmia.com/algorithms/nlp/LDA)\n",
    "    \n",
    "- **HDP**: Hierarchical Dirichlet Processes  \n",
    "    - [Hierarchical Dirichlet Processes](http://mlg.eng.cam.ac.uk/tutorials/07/ywt.pdf)\n",
    "- **Word2Vec**: Global Vectors for Word Representation\n",
    "    - [Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf)\n",
    "- **LogisticRegression**\n",
    "- **Naive Bayes**\n",
    "- **SVM**\n",
    "- **CountVectorizer**\n",
    "    - [Count Vectorizer ](https://de.dariah.eu/tatom/working_with_text.html)\n",
    "- **TfIdF**: Term frequency–Inverse document frequency\n",
    "    - [Term frequency–Inverse document frequency](https://nlp.stanford.edu/IR-book/html/htmledition/tf-idf-weighting-1.html)\n",
    "- **DTM**: Dynamic Topic Models\n",
    "    - [Dynamic Topic Models](https://radimrehurek.com/gensim/models/dtmmodel.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### NLP - simple example\n",
    "---\n",
    "Suppose we are building a spam/ham classifier. Input are emails, output is a binary classification.\n",
    "\n",
    "We can classify an email like the one below as spam - no spam by:\n",
    " \n",
    "> _Checking for the presence of the words Donate, WILL, sum, cancer, LinkedIn and similar._\n",
    "\n",
    "> Defining a simple rule that parses the text which is one of the simplest feature extraction from text: _binary word counting_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hello,\n",
      "I saw your contact information on LinkedIn. \n",
      "I have carefully read through your profile and you seem to have an outstanding personality. \n",
      "This is one major reason why I am in contact with you. My name is Mr. Valery Grayfer Chairman of the Board of \n",
      "Directors of PJSC \"LUKOIL\". I am 86 years old and I was diagnosed with cancer 2 years ago. I will be going \n",
      "in for an operation later this week. I decided to WILL/Donate the sum of 8,750,000.00 Euros\n",
      "(Eight Million Seven Hundred And Fifty Thousand Euros Only etc. etc.\n",
      "\n",
      "\n",
      "\n",
      "Hello,\n",
      "I am writing in regards to your application to the position of Data Scientist at Hooli X. \n",
      "We are pleased to inform you that you passed the first round of interviews and we would like to invite you \n",
      "for an on-site interview with our Senior Data Scientist Mr. John Smith. You will find attached to this message \n",
      "further information on date, time and location of the interview. Please let me know if I can be of any further \n",
      "assistance. Best Regards.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spam = \"\"\"\n",
    "Hello,\\nI saw your contact information on LinkedIn. \n",
    "I have carefully read through your profile and you seem to have an outstanding personality. \n",
    "This is one major reason why I am in contact with you. My name is Mr. Valery Grayfer Chairman of the Board of \n",
    "Directors of PJSC \"LUKOIL\". I am 86 years old and I was diagnosed with cancer 2 years ago. I will be going \n",
    "in for an operation later this week. I decided to WILL/Donate the sum of 8,750,000.00 Euros\n",
    "(Eight Million Seven Hundred And Fifty Thousand Euros Only etc. etc.\n",
    "\"\"\"\n",
    "\n",
    "ham = \"\"\"\n",
    "Hello,\\nI am writing in regards to your application to the position of Data Scientist at Hooli X. \n",
    "We are pleased to inform you that you passed the first round of interviews and we would like to invite you \n",
    "for an on-site interview with our Senior Data Scientist Mr. John Smith. You will find attached to this message \n",
    "further information on date, time and location of the interview. Please let me know if I can be of any further \n",
    "assistance. Best Regards.\n",
    "\"\"\"\n",
    "print spam\n",
    "print\n",
    "print ham\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### NLP - Bag of Words (word counting model, CountVectorizer, HashingVectorizer)\n",
    "---\n",
    "\n",
    "- The bag-of-words is a simplifying representation used in NLP. \n",
    "- In this method, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, **disregarding grammar and even word order but keeping multiplicity.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the word counting for Spam:\n",
      "Counter({'i': 7, 'of': 4, 'and': 3, 'is': 2, 'etc.': 2, 'am': 2, 'an': 2, 'have': 2, 'in': 2, 'your': 2, 'euros': 2, 'to': 2, 'years': 2, 'with': 2, 'this': 2, 'contact': 2, 'the': 2, 'major': 1, 'old': 1, 'cancer': 1, 'outstanding': 1, 'seven': 1, 'decided': 1, 'through': 1, 'carefully': 1, 'seem': 1, 'saw': 1, '(eight': 1, 'information': 1, 'for': 1, 'fifty': 1, '86': 1, 'sum': 1, '\"lukoil\".': 1, 'only': 1, 'pjsc': 1, 'mr.': 1, '2': 1, 'linkedin.': 1, 'will/donate': 1, 'you': 1, 'hundred': 1, 'was': 1, 'personality.': 1, 'chairman': 1, 'profile': 1, 'you.': 1, 'hello,': 1, 'ago.': 1, 'read': 1, 'going': 1, 'thousand': 1, 'million': 1, 'grayfer': 1, 'reason': 1, 'be': 1, 'one': 1, 'why': 1, 'on': 1, 'name': 1, 'week.': 1, '8,750,000.00': 1, 'later': 1, 'board': 1, 'operation': 1, 'will': 1, 'directors': 1, 'diagnosed': 1, 'valery': 1, 'my': 1})\n",
      "===========\n",
      "This is the word counting for Ham:\n",
      "Counter({'to': 5, 'you': 4, 'of': 4, 'the': 3, 'and': 2, 'we': 2, 'scientist': 2, 'data': 2, 'i': 2, 'further': 2, 'this': 1, 'regards.': 1, 'find': 1, 'information': 1, 'am': 1, 'an': 1, 'at': 1, 'in': 1, 'our': 1, 'message': 1, 'pleased': 1, 'best': 1, 'if': 1, 'will': 1, 'would': 1, 'with': 1, 'interviews': 1, 'please': 1, 'writing': 1, 'application': 1, 'mr.': 1, 'location': 1, 'passed': 1, 'interview': 1, 'for': 1, 'john': 1, 'date,': 1, 'be': 1, 'hello,': 1, 'x.': 1, 'invite': 1, 'that': 1, 'any': 1, 'interview.': 1, 'regards': 1, 'let': 1, 'know': 1, 'hooli': 1, 'on-site': 1, 'me': 1, 'on': 1, 'your': 1, 'like': 1, 'assistance.': 1, 'attached': 1, 'senior': 1, 'inform': 1, 'smith.': 1, 'can': 1, 'time': 1, 'position': 1, 'first': 1, 'round': 1, 'are': 1})\n"
     ]
    }
   ],
   "source": [
    "## Using Counter\n",
    "from collections import Counter\n",
    "print \"This is the word counting for Spam:\\n\" ,Counter(spam.lower().split())\n",
    "print \"===========\"\n",
    "print \"This is the word counting for Ham:\\n\", Counter(ham.lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##################\n",
    "## CountVectorizer : it count the frequency of each word.\n",
    "##################\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "## http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "'''CountVectorizer = Convert a collection of text documents to a matrix of token counts\n",
    "   tokens = (words, sentences, n-grams)'''\n",
    "\n",
    "cvec = CountVectorizer()\n",
    "cvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvec.fit([spam])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 69)\n",
      "\n",
      "(69, 1)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>of</th>\n",
       "      <th>and</th>\n",
       "      <th>your</th>\n",
       "      <th>contact</th>\n",
       "      <th>is</th>\n",
       "      <th>in</th>\n",
       "      <th>have</th>\n",
       "      <th>euros</th>\n",
       "      <th>the</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   of  and  your  contact  is  in  have  euros  the  this\n",
       "0   4    3     2        2   2   2     2      2    2     2"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Idea is to take the full result of words we have in a mess, and reorganize them in a table, and shape.\n",
    "df  = pd.DataFrame(cvec.transform([spam]).todense(),\n",
    "                   columns=cvec.get_feature_names())\n",
    "\n",
    "''' toarray returns an ndarray; \n",
    "    todense returns a matrix. \n",
    "    If you want a matrix, use todense; otherwise, use toarray\n",
    "'''\n",
    "print df.shape\n",
    "print ''\n",
    "\n",
    "'''getting the first 10 values'''\n",
    "print df.transpose().shape\n",
    "print ''\n",
    "df.transpose().sort_values(0, ascending=False).head(10).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### NLP - Hashing Vectorizer\n",
    "---\n",
    "- When using the `CountVectorizer` method we need to set up a dictionary to have a fixed size, only keeping words of certain frequencies.\n",
    "- But, we still have to compute a dictionary and hold the dictionary in memory. This could be a **problem** when we have a large corpus or in streaming applications where we don't know which words we will encounter in the future.\n",
    "\n",
    "\n",
    "- **Solution:** Use the `HashingVectorizer`, which converts a collection of text documents to a matrix of occurrences, calculated with the [hashing trick](https://en.wikipedia.org/wiki/Feature_hashing). Each word is mapped to a feature with the use of a [hash function](https://en.wikipedia.org/wiki/Hash_function) that converts it to a hash. If we encounter that word again in the text, it will be converted to the same hash, allowing us to count word occurence without retaining a dictionary in memory. This is very convenient!\n",
    "\n",
    "\n",
    "- **The problem:** with this trick is that it's not possible to compute the inverse transform, and thus we lose information on what words the important features correspond to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HashingVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "         dtype=<type 'numpy.float64'>, encoding=u'utf-8', input=u'content',\n",
       "         lowercase=True, n_features=1048576, ngram_range=(1, 1),\n",
       "         non_negative=False, norm=u'l2', preprocessor=None,\n",
       "         stop_words=None, strip_accents=None,\n",
       "         token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####################\n",
    "## HashingVectorizer\n",
    "####################\n",
    "\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "hvec = HashingVectorizer()\n",
    "hvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HashingVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "         dtype=<type 'numpy.float64'>, encoding=u'utf-8', input=u'content',\n",
       "         lowercase=True, n_features=1048576, ngram_range=(1, 1),\n",
       "         non_negative=False, norm=u'l2', preprocessor=None,\n",
       "         stop_words=None, strip_accents=None,\n",
       "         token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hvec.fit([spam])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>479532</th>\n",
       "      <th>144749</th>\n",
       "      <th>174171</th>\n",
       "      <th>832412</th>\n",
       "      <th>828689</th>\n",
       "      <th>994433</th>\n",
       "      <th>1005907</th>\n",
       "      <th>170062</th>\n",
       "      <th>675997</th>\n",
       "      <th>959146</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.338062</td>\n",
       "      <td>0.169031</td>\n",
       "      <td>0.169031</td>\n",
       "      <td>0.169031</td>\n",
       "      <td>0.169031</td>\n",
       "      <td>0.169031</td>\n",
       "      <td>0.169031</td>\n",
       "      <td>0.169031</td>\n",
       "      <td>0.169031</td>\n",
       "      <td>0.084515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    479532    144749    174171    832412    828689    994433    1005907  \\\n",
       "0  0.338062  0.169031  0.169031  0.169031  0.169031  0.169031  0.169031   \n",
       "\n",
       "    170062    675997    959146   \n",
       "0  0.169031  0.169031  0.084515  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df  = pd.DataFrame(hvec.transform([spam]).todense())\n",
    "df.transpose().sort_values(0, ascending=False).head(10).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### NLP - Bag of Words (drawbacks)\n",
    "---\n",
    "\n",
    "- Completely ignores the structure of a sentence. Bag of Words\n",
    "- Bag of Words merely assess presence of specific words or word combinations.\n",
    "- The same word can have multiple meanings in different contexts. Consider for example the following two sentences:\n",
    "\n",
    "    - There's wood floating in the **sea**\n",
    "    - Mike's in a **sea** of trouble with the move\n",
    "\n",
    "In the first case the word \"sea\" indicates a large body of water, while in the second case it indicates \"a lot of\".\n",
    "\n",
    "How do we teach a computer to disambiguate? Here are some additional techniques that may come to help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### NLP - Segmentation (hard way and easy way with NLTK)\n",
    "---\n",
    "\n",
    "This is a technique to **identify sentences** within a body of text. \n",
    "As we know, language is not a continuous uninterrupted stream of words: punctuation serves as a guide to group together words that convey meaning when contiguous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I went to the zoo today.',\n",
       " ' What do you think of that?',\n",
       " ' I bet you hate it!']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####################\n",
    "## Segmentation\n",
    "## The hard way!!\n",
    "####################\n",
    "\n",
    "easy_text = \"I went to the zoo today. What do you think of that? I bet you hate it! Or maybe you don't\"\n",
    "\n",
    "def simple_sentencer(text):\n",
    "    '''take a string called `text` and return\n",
    "    a list of strings, each containing a sentence'''\n",
    "\n",
    "    sentences = []\n",
    "    substring = ''\n",
    "    for c in text:\n",
    "        if c in ('.', '!', '?'):\n",
    "            sentences.append(substring + c)\n",
    "            substring = ''\n",
    "        else:\n",
    "            substring += c\n",
    "    return sentences\n",
    "\n",
    "simple_sentencer(easy_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "##################################\n",
    "''' Is NLTK install in my computer?\n",
    "    If this opens, then you need to install \n",
    "    packages as you need''' \n",
    "################################\n",
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I went to the zoo today.',\n",
       " 'What do you think of that?',\n",
       " 'I bet you hate it!',\n",
       " \"Or maybe you don't\"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##########################\n",
    "## Segmentation with NLTK\n",
    "#########################\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "detect_sentence = PunktSentenceTokenizer()\n",
    "\n",
    "detect_sentence.sentences_from_text(easy_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### NLP - with NLTK (Normalization)\n",
    "---\n",
    "\n",
    "- **_Normalization_: Text normalization is the process of transforming text into a single canonical form that it might not have had before.**\n",
    "\n",
    "\n",
    "- **_Normalization_** is when slightly different version of a word exist. For example: LinkedIn sees 6000+ variations of the title \"Software Engineer\" and 8000+ variations of the word \"IBM\".\n",
    "- Here is a bigger list:\n",
    "    - Person titles (Mr. MR. DR etc.)\n",
    "    - Dates (10/03, March 10 etc.)\n",
    "    - Numbers\n",
    "    - Plurals\n",
    "    - Verb conjugations\n",
    "    - Slang\n",
    "    - SMS abbreviations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### NLP - with NLTK (Stemming)\n",
    "---\n",
    "- **_Stemming_ is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form—generally a written word form**\n",
    "\n",
    "\n",
    "- **_Stemming_** It would be wrong to consider the words \"MR.\" and \"mr\" to be different features, thus we need a technique to normalize words to a common root. This technique is called _Stemming_.\n",
    "\n",
    "- Science, Scientist => Scien\n",
    "- Swimming, Swimmer, Swim => Swim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####################\n",
    "## Stemming\n",
    "## The hard way and \n",
    "## really innefective\n",
    "####################\n",
    "def stem(tokens):\n",
    "    '''rules-based stemming of a bunch of tokens'''\n",
    "\n",
    "    new_bag = []\n",
    "    for token in tokens:\n",
    "        # define rules here\n",
    "        if token.endswith('s'):\n",
    "            new_bag.append(token[:-1])\n",
    "        elif token.endswith('er'):\n",
    "            new_bag.append(token[:-2])\n",
    "        elif token.endswith('tion'):\n",
    "            new_bag.append(token[:-4])\n",
    "        elif token.endswith('tist'):\n",
    "            new_bag.append(token[:-4])\n",
    "        elif token.endswith('ce'):\n",
    "            new_bag.append(token[:-2])\n",
    "        elif token.endswith('ing'):\n",
    "            new_bag.append(token[:-2])\n",
    "        else:\n",
    "            new_bag.append(token)\n",
    "\n",
    "    return new_bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Swimmed', 'Swimmi']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem(['Swimmed', 'Swimming'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Swim\n",
      "Swim\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "## Stemming with NLTK\n",
    "#########################\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "print stemmer.stem('Swimmed')\n",
    "print stemmer.stem('Swimming')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### NLP - with NLTK (Stop Words)\n",
    "---\n",
    "- Some words are very common and provide **NO** information on the text content, and should be removed!.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "## Stop Words with NLTK\n",
    "#########################\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "print \"There are\", len(stop), \"in the english nltk program\"\n",
    "print ''\n",
    "sentence = \"this class is great is the best way to learn ml and python\"\n",
    "print sentence.split()\n",
    "print ''\n",
    "print [i for i in sentence.split() if i not in stop] #print words of each sentence that are not in stop words list\n",
    "\n",
    "#instead of not in we can print the list of words that are in stop word list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### NLP - with NLTK (Parts of Speech)\n",
    "---\n",
    "- We all know that each word has a specific role in a sentence (Verb, Noun etc.) \n",
    "- Parts-of-speech tagging (POS) is a **feature extraction technique** that attaches a _tag_ to each word in the sentence, to provide a more precise context for further analysis. \n",
    "- This is often a resource intensive process, but it can sometimes improve the accuracy of models.\n",
    "- [What are all possible pos tags of NLTK?](http://stackoverflow.com/questions/15388831/what-are-all-possible-pos-tags-of-nltk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('this', 'DT'),\n",
       " ('class', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('great', 'JJ'),\n",
       " ('is', 'VBZ'),\n",
       " ('the', 'DT'),\n",
       " ('best', 'JJS'),\n",
       " ('way', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('learn', 'VB'),\n",
       " ('ml', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('python', 'NN')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############################\n",
    "## Parts of Speech with NLTK\n",
    "############################\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "tok = WordPunctTokenizer()\n",
    "pos_tag(tok.tokenize(\"this class is great is the best way to learn ml and python\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### NLP - with NLTK (Term frequency - Inverse document Frequency)\n",
    "---\n",
    "\n",
    "- Tf-Idf will give you an score that shows which words are **most discriminating** between documents. Words that occur a lot in one document but doesn't occur in many documents will tell you something special about the document.\n",
    "\n",
    "- This weight/score evaluates how important a word is to a document in a collection (corpus)\n",
    "- The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus.\n",
    "- [Here is a deeper explanation for tf–idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)\n",
    "\n",
    "\n",
    "Term frequency tf is the frequency of a certain term in a document:\n",
    "\n",
    "$$\n",
    "\\mathrm{tf}(t,d) = \\frac{N_\\text{term}}{N_\\text{terms in Document}}\n",
    "$$\n",
    "\n",
    "Inverse document frequency is defined as the frequency of documents that contain that term over the whole corpus.\n",
    "\n",
    "$$\n",
    "\\mathrm{idf}(t, D) = \\log\\frac{N_\\text{Documents}}{N_\\text{Documents that contain term}}\n",
    "$$\n",
    "\n",
    "Term frequency - Inverse Document Frequency is calculated as:\n",
    "\n",
    "$$\n",
    "\\mathrm{tfidf}(t,d,D) = \\mathrm{tf}(t,d) \\cdot \\mathrm{idf}(t, D)\n",
    "$$\n",
    "\n",
    "\n",
    "<Br><br>\n",
    "![](https://snag.gy/rBNLtd.jpg)\n",
    "\n",
    "This enhances terms that are highly specific of a particular document, while suppressing terms that are common to most documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################################\n",
    "## Term frequency - Inverse document Frequency with NLTK\n",
    "########################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "## http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "tvec = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "tvec.fit([spam, ham])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>years</th>\n",
       "      <th>euros</th>\n",
       "      <th>contact</th>\n",
       "      <th>personality</th>\n",
       "      <th>linkedin</th>\n",
       "      <th>lukoil</th>\n",
       "      <th>major</th>\n",
       "      <th>million</th>\n",
       "      <th>old</th>\n",
       "      <th>operation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>0.290133</td>\n",
       "      <td>0.290133</td>\n",
       "      <td>0.290133</td>\n",
       "      <td>0.145067</td>\n",
       "      <td>0.145067</td>\n",
       "      <td>0.145067</td>\n",
       "      <td>0.145067</td>\n",
       "      <td>0.145067</td>\n",
       "      <td>0.145067</td>\n",
       "      <td>0.145067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         years     euros   contact  personality  linkedin    lukoil     major  \\\n",
       "spam  0.290133  0.290133  0.290133     0.145067  0.145067  0.145067  0.145067   \n",
       "ham   0.000000  0.000000  0.000000     0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "       million       old  operation  \n",
       "spam  0.145067  0.145067   0.145067  \n",
       "ham   0.000000  0.000000   0.000000  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df  = pd.DataFrame(tvec.transform([spam, ham]).todense(),\n",
    "                   columns=tvec.get_feature_names(),\n",
    "                   index=['spam', 'ham'])\n",
    "print len(df)\n",
    "print ''\n",
    "df.transpose().sort_values('spam', ascending=False).head(10).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>regards</th>\n",
       "      <th>interview</th>\n",
       "      <th>data</th>\n",
       "      <th>scientist</th>\n",
       "      <th>location</th>\n",
       "      <th>position</th>\n",
       "      <th>let</th>\n",
       "      <th>know</th>\n",
       "      <th>senior</th>\n",
       "      <th>invite</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>0.31039</td>\n",
       "      <td>0.31039</td>\n",
       "      <td>0.31039</td>\n",
       "      <td>0.31039</td>\n",
       "      <td>0.155195</td>\n",
       "      <td>0.155195</td>\n",
       "      <td>0.155195</td>\n",
       "      <td>0.155195</td>\n",
       "      <td>0.155195</td>\n",
       "      <td>0.155195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      regards  interview     data  scientist  location  position       let  \\\n",
       "spam  0.00000    0.00000  0.00000    0.00000  0.000000  0.000000  0.000000   \n",
       "ham   0.31039    0.31039  0.31039    0.31039  0.155195  0.155195  0.155195   \n",
       "\n",
       "          know    senior    invite  \n",
       "spam  0.000000  0.000000  0.000000  \n",
       "ham   0.155195  0.155195  0.155195  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.transpose().sort_values('ham', ascending=False).head(10).transpose()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
