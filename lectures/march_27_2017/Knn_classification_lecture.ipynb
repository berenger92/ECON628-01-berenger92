{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://images.efollett.com/htmlroot/images/templates/storeLogos/CA/864.gif\" style=\"float: right;\"> \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ECON628-01 \n",
    "### Lecture 1.2 - K-nearest neighbors (KNN - classifier)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Leaning - KNN\n",
    "---\n",
    "- The models we have covered are probabilistic models of the form  $p(y\\;|\\;x)$ which are supervised methods, we will cover unsupervised probabilistic models $p(\\;x\\;)$ later in the class\n",
    "$~$\n",
    "\n",
    " \n",
    "- Although, there are many ways to define these models, one crucial distinction is if the model have a fixed number of parameters, or does the number of parameters grow with the amount of training data?\n",
    "$~$\n",
    "\n",
    "\n",
    "- Meaning is it a parametric model, or a non- parametric model. \n",
    "$~$\n",
    "\n",
    "\n",
    "- Remember that parametric models have the advantage of often being faster to use, but the disadvantage of making stronger assumptions about the nature of the data distributions. \n",
    "$~$\n",
    "\n",
    "\n",
    "- On the contrary, non-parametric models are more flexible, but are often computationally intractable for large datasets. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Leaning - KNN\n",
    "---\n",
    "- K nearest neighbors is the most simple non-parametric classifier => has been used in statistical estimation and pattern recognition already in the beginning of 1970’s as a non-parametric technique.\n",
    "\n",
    "- K nearest neighbors is a simple algorithm that stores all available cases and classifies new cases based on a similarity measure (e.g., distance functions)\n",
    "\n",
    "- In other words: When classifying a new example, find $k$ nearest training example, and assign the majority label\n",
    "![](https://snag.gy/tWIZzA.jpg)\n",
    "*Images from Python Machine Learning*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Leaning - KNN\n",
    "---\n",
    "\n",
    "- It is considered a lazy learning algorithm because the model doesn't learn a **discriminative function** from the training data but memorizes the training data instead. \n",
    "    - Discriminative function analysis: use to determine which variables discriminate between two or more naturally occurring groups. \n",
    "        - For example, a researcher may want to investigate which variables discriminate between high school graduates who decide (1) to go to college, (2) to attend a trade or professional school, or (3) to seek no further training or education. \n",
    "        - For that purpose the researcher could collect data on numerous variables prior to students' graduation. \n",
    "        - After graduation, most students will naturally fall into one of the three categories. \n",
    "        - **Discriminant Analysis** could then be used to determine which variable(s) are the best predictors of students' subsequent educational choice. \n",
    "        \n",
    "        \n",
    "- Also, lazy learning algorithm means that the model does not use the training data points to do any generalization (there is no explicit training phase or it is very minimal) => this makes the training part fast.\n",
    "- Since it doesn't generalized, KNN keeps all the training data => this means that all the training data is needed during the testing phase. \n",
    "- However, the tradeoff between a very minimal training phase is a costly (in terms of time and memory) testing phase\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Leaning - KNN\n",
    "---\n",
    "#### Assumptions\n",
    "---\n",
    "- Data is in a feature space, this means that the data points are in a metric space, or have a notion of distance. \n",
    "\n",
    "\n",
    "- Each training data consists of a set of vectors and class label associated with each vector. \n",
    "    - Where the labels can be + or – (for positive or negative classes), or it can be an arbitrary number of classes.\n",
    "\n",
    "\n",
    "- We are given a single number \"k\", that decides how many neighbors (where neighbors is defined based on the distance metric) influence the classification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Leaning - KNN\n",
    "---\n",
    "#### The Classification\n",
    "---\n",
    "\n",
    "- A unknown case will be classified by a majority vote of its neighbors, then the unknown case will be assigned to the **class most common** amongst its K nearest neighbors measured by a **distance function.** \n",
    "&nbsp;\n",
    "- If K = 1, then the case is simply assigned to the class of its nearest neighbor. \n",
    "![](https://snag.gy/qsz20D.jpg)\n",
    "*Images from Siddharth Deokar*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Leaning - KNN\n",
    "---\n",
    "#### Distance Metrics and choosing K\n",
    "---\n",
    "$$ \\text{Euclidian} = \\sqrt {\\sum_{i=1}^k \\; (x_i \\;-\\; y_i)^2} $$\n",
    "\n",
    "$$ \\text{Manhattan} = \\sum_{i=1}^k \\; \\left\\lvert x_i \\;-\\; y_i\\right\\rvert $$\n",
    "\n",
    "$$ \\text{Minkowski} = \\left( \\sum_{i=1}^k \\; (\\; \\left\\lvert x_i \\;-\\; y_i\\right\\rvert \\; )^q \\right)^{1/q} $$\n",
    "\n",
    "$~$\n",
    "- Choosing the correct k is fundamental to find a good balance between over and underfitting\n",
    "\n",
    "\n",
    "- Choosing the distance metric is related to the features on the dataset (Euclidian is used for real-values samples)\n",
    "\n",
    "\n",
    "- When using the Euclidian distance, we need to standarized the data => each future contributes equally to the distance.\n",
    "\n",
    "\n",
    "- The Minkowski distance is a generalization of the Euclidian and Manhattan distances\n",
    "\n",
    "\n",
    "- And there are more distances http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Leaning - KNN\n",
    "---\n",
    "#### The curse of dimensionality\n",
    "---\n",
    "- The KNN classifier is simple and can work quite well, provided it is given a **good distance metric** and has **enough labeled training data.**\n",
    "- However KNN is extremely susceptible to overfitting because of the **curse of dimensionality.**\n",
    "- Curse of dimensionality => describes the issue on which the feature space becomes increasingly scattered for an increasing number of dimensions of a fixed-size training dataset.\n",
    "- In turn, high dimensionality makes clustering hard, because having lots of dimensions means that everything is \"far away\" from each other. In this escenario it's hard to know what true distance means when you have so many dimensions. - It is is here where it's often helpful to perform PCA or feature selection to reduce dimensionality before clustering.\n",
    "- You can't use regularization methods with KNN or decision trees\n",
    "- This is a [link][1] to a great tutorial\n",
    "\n",
    "[1]: https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor/ \"link\""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
