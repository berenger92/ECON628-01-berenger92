{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://images.efollett.com/htmlroot/images/templates/storeLogos/CA/864.gif\" style=\"float: right;\"> \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ECON628-01 \n",
    "### Lecture 1.1 - Machine Learning \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Leaning\n",
    "* A field that originated from computer science (especially, artificial intelligence)\n",
    "* It has been influenced by statistics in the past 15 years or so, therefore many of the concepts are not entirely new, but instead, they are called something else"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Leaning\n",
    "* When humans improve their skills => they are said to learn\n",
    "* Can you program a computer to do the same?\n",
    " *  Arthur Samuel was the pioneer in this are => programming a computer to play checkers, and coined the term **Machine Learning ML** in 1959. \n",
    " * The computer play against itself and human opponents => improving the performance with every game\n",
    " * Eventually after sufficient training, the computer became a better player that the human progammer\n",
    "* **ML** has become and interdisciplinary field (Computer Science, artificiall intelligence, databases and statistics)\n",
    "* **ML** seeks to design computer systems that improve over time with more experience\n",
    "* **ML** goal is not just to find the best function $F$ that can predict $Y$, but to find that one that best **generalized to unseen data**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Leaning  - Commercial Applications (short list of examples)\n",
    "* Speech Recognition => NLP, Neural Networks (Alexa, Siri)\n",
    "* Autonomous cars => Neural Networks (Ford, Google, Uber)\n",
    "* Fraud Detection => Classification (Banks)\n",
    "* Personalized ads => Recommenders (Netflix, Amazon, Stich FIx)\n",
    "* Face/image recognition => Survellience systems, social networks, farming, poverty alleviation\n",
    "    * [Metabiota] [1]\n",
    "    * [Premise] [2]\n",
    "[1]: http://metabiota.com/   \"Metabiota\"\n",
    "[2]: http://www.premise.com/ \"Premise\"   \n",
    "    \n",
    "  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Leaning - The process\n",
    "* When solving problems using ML methods, the researcher needs to think of the larger **_data-driven problem-solving_** process of which these methods are a small part \n",
    " 1. **Understand the problem and goal** => Move from a vague premise to a concrete analytical formulation\n",
    " 2. **Formulate it as a ML problem** => classification or regression problem, build a model that detects anomalies as new data come in\n",
    " 3. **Data exploration and preparation** => EDA (exploratory data analysis), you need to carefully explore your data (missing values, need additional data)\n",
    " 4. **Feature engineering** => features = independent variables, predictors, factors, covariates. Feature transformation, interactions, aggregation of data over time and space\n",
    " 5. **Method Selection** => in ML you take a collection of methods and try them out empirically then validata which one works the best for your problem \n",
    " 6. **Evaluation** => Need to select the model that is the best (Model Evaluation process)\n",
    " 7. **Deployment** of model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Leaning - Methods\n",
    "* In general there are 2 major categories\n",
    "    1. **Supervised Learning methods** => problem when there is a target variable (continuos or discrete) that we want to predict or classify data into.\n",
    "        * Formally, this SLN methods predict a value of $Y$ given input(s) $X$ by learning (estimating, fitting or training) a function $F$, where $F(X) = Y$ and $X$ is the set of features and $Y$ is the label (target/dependent variable) \n",
    "        * Thus the goal is to search for that function $F$ that best predicts $Y$. (if $Y$ categorical then classfication, if $Y$ continous then regression)\n",
    "        \n",
    "    2. **Unsupervised Learning methods** => problem where there does not exist a taget variable that we want to predict but we want to understand _natural_ grouping patterns in the data.\n",
    "    \n",
    "**_Supervision_** in this escenario is the presence of target variables (labels). Unsupervised = none of the data points have labels. Supervised = all the data points have labels\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Dos](https://snag.gy/EnuRi2.jpg)\n",
    "*Image from BMVA summer school 2014 *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Leaning - Vocabulary\n",
    "* **Learning** => Use in the context of learning a model. Similar to _fitting_, or _estimating a function_, or _training_, or _building a model_\n",
    "* **Examples** => This are the data points and instances\n",
    "* **Features** => Independent variables, attributes, predictor variables, and explanatory variables\n",
    "* **Labels** => Response variable, dependent variable, and target variable\n",
    "* **Underfitting** => Model is to simple and doesn't capture the structure of the data well enough\n",
    "* **Overfitting** => Model is possibly too complex and models the noise in the data, which can result in poor generalization performance\n",
    "* **Regularization** => General method to avoid overfitting by applying additional constraints to the model that is learned. Two common regularization are $L_1$ = Lasso and $L_2$ = Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Leaning - Econometrics\n",
    "\n",
    "* Machine learning methods are about algorithms, more than about asymptotic statistical properties. No unified framework, like maximum likelihood estimation.\n",
    "* Setting is one with large data sets, sometimes many units, sometimes many predictors. Scalability is a big issue\n",
    "* **Causality is de-emphasized.** Methods are largely about _prediction and fit_. That is important in allowing validation of methods through out-of-sample crossvalidation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Leaning - Components\n",
    "**1. Evaluation (Out-of-sample and Cross-validation):** Methods are validated by assessing their properties out of sample.\n",
    "* Much easier for prediction problems than for causal problems. For prediction problems we see realizations so that a **single** observation can be used to estimate the quality of the prediction: a single realization of ($Y_i,X_i$) gives us an unbiased estimate of $μ(x) = E[Y_i|X_i = x]$, namely $Y_i$.\n",
    "* For causal problems we do not generally have unbiased estimates of the true causal effects.\n",
    "* Use “training sample” to “train” (estimate) model, and “test sample” to compare algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Leaning - Components\n",
    "**1.1 Evaluation Methodologies**\n",
    "* **Out-of-sample and holdout test**: goal here is to pretent to generalize to new (unseen data). You do this by taking the original dataset and randomly split them into 2 dataset _train_ data and the _test_ data (the _test_ data is also called the _holdout_ or _validation set_)\n",
    "* We can decide how much we want to keep on each set (typically, 50:50, 80:20, 70:30 depending on the size of your dataset)\n",
    "* Then we train our models on the training data and classify/predict the data on the test data => it gives us an estimate of the relative performance of the methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Leaning - Components\n",
    "**1.1 Evaluation Methodologies**\n",
    "* **Cross validation** Begins by splitting a label data set into $k$ partitions (called folds), where $k$ is typically set to 5 or 10. Cross validation then proceeds by iterating $k$ times. In each iteration one of the $k$ folds is held put as the test set, while the other $k$-1 folds are combined and used to train the model.\n",
    "* **Pros**\n",
    "    * Every example is used in one test set for testing the model\n",
    "    * Each iteration gives us a performance estimate that can be aggregated/averaged to generate the overall estimate\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Leaning - train and test set\n",
    "\n",
    "![Dos](https://snag.gy/rm0y8P.jpg)\n",
    "*Image from an Introduction to Statistical Learning*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Leaning - Out of sample and holdout\n",
    "\n",
    "![Dos](https://snag.gy/YyRIjZ.jpg)\n",
    "*Image from an Introduction to Statistical Learning*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Leaning - 5 fold cross validation\n",
    "\n",
    "![Dos](https://snag.gy/5gHmjX.jpg)\n",
    "*Image from an Introduction to Statistical Learning*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Leaning - Prediction\n",
    "** Prediction**\n",
    "* Supposed that a set of inputs $X$ are available but the output $Y$ cannot be easly obtained, thus the error term averages to zero, we can predict $Y$ using $\\hat Y$ = $\\hat f(X)$.\n",
    "Where $\\hat f$ represents the estimate for $f$ and $\\hat Y$ represents the resulting prediction for $Y$\n",
    "* $\\hat f$ is treated as a black box, as we are not concern with the exact form of $\\hat f$, provided that it yields accurate predictions of $Y$\n",
    "* The accuracy of $\\hat Y$ as a predictor of $Y$ depens on the _reducible error_ and _irreducible error_\n",
    "* Generally $\\hat f$ will not be a perfect estimate for $f$ this innacuracy introduces some error which is **_reducible_** because we can potentially improve the accuracy of $\\hat f$ by using the most appropriate statistical learning technique to estimate $f$\n",
    "* But $Y$ is also a function of $ε$, which **cannot** be predicted using $X$. Therefore, variability associated with $ε$ also affects the accuracy of our predictions. This is known as the _irreducible error_, because no matter how well we estimate $f$, we cannot reduce the error introduced by $ε$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Leaning - Prediction\n",
    "** Prediction**\n",
    "\n",
    "$$E(Y − \\hat Y)^2 = E[f(X)+ε−f(X)]^2$$\n",
    "\n",
    "$$E(Y − \\hat Y)^2 = {[f(X)−f(X)]} + Var(ε) => Reducible + Irreducible$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Leaning - Parametric Methods - OLS\n",
    "1. Parametric methods involve a two-step model-based approach:\n",
    "\n",
    "    1.1 Make an assumption about the functional form, or shape, of $f$ (OLS => i.e. $f$ is linear in $X$)\n",
    "$$ f(X)=β_0 +β_1X_1 +β_2X_2 +...+β_pX_p.$$\n",
    "    \n",
    "    1.2 Once a model is selected we need a procedure that uses the training data to _fit_ or train the model In the case of our linear model we need to estimate parameters $β_0$+$β_1$+$β_2$+...+$β_p$. This means that we need to find values of these parameters so that\n",
    "    \n",
    "$$Y ≈β_0 +β_1X_1 +β_2X_2 +...+β_pX_p$$\n",
    "\n",
    "where “≈” as “is approximately modeled as”.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Leaning - OLS - Assesing the accuracy of the model\n",
    "* Let $\\hat Y_i$ = $\\hat β_0$ + $\\hat β_1x_i$ be the prediction for $Y$ based on the _i_th value of $X$. \n",
    "* Then $e_i$ = $y_i$ − $\\hat y_i$ represents the _i_th residual => this is the difference between the _i_th observed response value and the _i_th response value that is predicted by a  linear model\n",
    "* We define the residual sum of squares (RSS) as\n",
    "$$R S S = e^2_1 + e^2_2 + · · · + e^2_n$$\n",
    "\n",
    "* Residual Standard Error (RSE), associated with each observation is an error term $ε$, and is because the presence of the error terms that we are unable to predict $Y$ from $X$\n",
    "* RSE is an estimate of the standard deviation of $ε$, thus RSE is\n",
    "$$RSE = \\frac{\\sqrt 1} {(\\sqrt n-\\sqrt2)} {\\sqrt RSS}$$\n",
    "\n",
    "* Mean Square Error (MSE) => average of the sum of the square errors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Leaning - Regularization\n",
    "Rather than simply choosing the best fit, there is some penalty to avoid over-fitting.\n",
    "\n",
    "• Two issues: choosing the form of the regularization, and choosing the amount of regularization.\n",
    "\n",
    "* Traditional methods in econometrics often used plug-in methods: use the data to estimate the unknown functions and express the optimal penalty term as a function of these quantities. \n",
    "* The machine learning literature has focused on out-of-sample cross-validation methods for choosing amount of regularization (value of penalty).\n",
    "* Sometimes there are multiple tuning parameters, and more structure needs to be imposed on selection of tuning parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Learning - the least squares loss function\n",
    "\n",
    "You've become familiar at this point with the least squares loss function. Vanilla regression minimizes the residual sum of squares (RSS) to fit the data:\n",
    "\n",
    "$$ \\text{minimize}\\; RSS = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^n \\left(y_i - \\left(\\beta_0 + \\sum_{j=1}^p\\beta_j x_j\\right)\\right)^2 $$\n",
    "\n",
    "Where our model predictions for $y$ are based on the sum of the $beta_0$ intercept and the products of $\\beta_i$ with $x_i$.\n",
    "\n",
    "[Loss Function] [1]\n",
    "[1]: https://davidrosenberg.github.io/ml2015/docs/3a.loss-functions.pdf \"Loss Function\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Learning - Ridge regression\n",
    "\n",
    "The Ridge regression adds an additional thing to the loss function: the sum of the squared (non-intercept!) $\\beta$ values:\n",
    "$$ \\text{minimize}\\; RSS+Ridge = \\sum_{i=1}^n \\left(y_i - \\left(\\beta_0 + \\sum_{j=1}^p\\beta_j x_j\\right)\\right)^2 + \\lambda_2\\sum_{j=1}^p \\beta_j^2$$\n",
    "\n",
    "What are these new components?\n",
    "\n",
    "$\\beta_j^2$ is the squared coefficient for variable $x_j$.\n",
    "\n",
    "$\\sum_{j=1}^n \\beta_ij^2$ is the sum of these squared coefficients for every variable we have in our model. This does **not** include the intercept $\\beta_0$.\n",
    "\n",
    "$\\lambda_2$ is a constant for the _strength_ of the regularization parameter. The higher this value, the greater the impact of this new component in the loss function. If this were zero, then we would revert back to just the least squares loss function. If this were, say, a billion, then the residual sum of squares component would have a much smaller effect on the loss/cost than the regularization term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Learning - Lasso regression\n",
    "\n",
    "The Lasso regression takes a different approach. Instead of adding the sum of _squared_ $\\beta$ coefficients to the RSS, it adds the sum of the _absolute value_ of the $\\beta$ coefficients:\n",
    "\n",
    "$$ \\text{minimize}\\; RSS + Lasso = \\sum_{i=1}^n \\left(y_i - \\left(\\beta_0 + \\sum_{j=1}^p\\beta_j x_j\\right)\\right)^2 + \\lambda_1\\sum_{j=1}^p |\\beta_j|$$\n",
    "\n",
    "$|\\beta_j|$ is the absolute value of the $\\beta$ coefficient for variable $x_j$\n",
    "\n",
    "$\\lambda_1$ is again the strength of the regularization penalty component in the loss function. In lasso the lambda is denoted with a 1, in ridge the lambda is denoted with a 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Learning - Elastic Net\n",
    "\n",
    "Elastic Net is a combination of both the Lasso and the Ridge regularizations. It adds both penalties to the loss function:\n",
    "\n",
    " $$ \\text{minimize}\\; RSS + Ridge + Lasso = \\sum_{i=1}^n \\left(y_i - \\left(\\beta_0 + \\sum_{j=1}^p\\beta_j x_j\\right)\\right)^2 + \\lambda_1\\sum_{j=1}^p |\\beta_j| + \\lambda_2\\sum_{j=1}^p \\beta_j^2$$\n",
    "\n",
    "In the elastic net, the effect of the Ridge vs. the Lasso is balanced by the two lambda parameters. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Learning - so when do you use each? what is the effect of regularization?\n",
    "\n",
    "When you have **multicollinearity** in the data.\n",
    "\n",
    "The Lasso and Elastic Net are also very useful for when you have redundant or unimportant variables. If you have 1000 variables in a dataset the Lasso can perform \"feature selection\" automatically for you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Learning - normalize predictors \n",
    "With the Lasso and Ridge it is neccessary to normalize the predictor columns before constructing models, even the dummy coded categorical variables. \n",
    "\n",
    "\n",
    "#### Why is normalization of predictors required?\n",
    "\n",
    "Recall the equations for the Ridge and Lasso penalties:\n",
    "\n",
    "$$ \\text{Ridge penalty}\\; = \\lambda_2\\sum_{j=1}^p \\beta_j^2$$\n",
    "\n",
    "$$ \\text{Lasso penalty}\\; = \\lambda_2\\sum_{j=1}^p |\\beta_j|$$\n",
    "\n",
    "**How are the $\\beta$ coefficients affected by the mean and variance of your variables?**\n",
    "\n",
    "If the mean and variance of your $x$ predictors are different, their respective $\\beta$ coefficients _scale with the mean and variance of the predictors **regardless of their explanatory power.**_\n",
    "\n",
    "This means that if one of your $x$ variables, for example the price of a home, will have a much smaller $\\beta$ value than say the number of bedrooms in a house – just because the scale of the two variables are so different.\n",
    "\n",
    "The Ridge and Lasso penalties are agnostic to the mean and variance of your predictors. All they \"care about\" are the values of the coefficients. If one of your coefficients is much larger than any of the others, it will dominate the effect of the penalty on your minimization!\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
