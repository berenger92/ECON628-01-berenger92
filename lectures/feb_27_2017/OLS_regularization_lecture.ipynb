{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://images.efollett.com/htmlroot/images/templates/storeLogos/CA/864.gif\" style=\"float: right;\"> \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ECON628-01 \n",
    "### Lecture 1.1 - Prediction, OLS and Regularization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Leaning - Prediction\n",
    "** Prediction**\n",
    "* Supposed that a set of inputs $X$ are available but the output $Y$ cannot be easly obtained, thus the error term averages to zero, we can predict $Y$ using $\\hat Y$ = $\\hat f(X)$.\n",
    "Where $\\hat f$ represents the estimate for $f$ and $\\hat Y$ represents the resulting prediction for $Y$\n",
    "* $\\hat f$ is treated as a black box, as we are not concern with the exact form of $\\hat f$, provided that it yields accurate predictions of $Y$\n",
    "* The accuracy of $\\hat Y$ as a predictor of $Y$ depens on the _reducible error_ and _irreducible error_\n",
    "* Generally $\\hat f$ will not be a perfect estimate for $f$, this innacuracy introduces some error which is **_reducible_** because we can potentially improve the accuracy of $\\hat f$ by using the most appropriate statistical learning technique to estimate $f$\n",
    "* But $Y$ is also a function of $ε$, which **cannot** be predicted using $X$. Therefore, variability associated with $ε$ also affects the accuracy of our predictions. This is known as the **_irreducible error_**, because no matter how well we estimate $f$, we cannot reduce the error introduced by $ε$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Machine Leaning - Prediction\n",
    "** Prediction**\n",
    "\n",
    "$$E(Y − \\hat Y)^2 = E[f(X)+ε− \\hat f(X)]^2$$\n",
    "\n",
    "$$E(Y − \\hat Y)^2 = {[f(X)−\\hat f(X)]} + Var(ε) => Reducible + Irreducible$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Leaning - Parametric Methods - OLS\n",
    "In ML **Causality is de-emphasized.** Methods are largely about **_prediction and fit_.** That is important in allowing validation of methods through out-of-sample crossvalidation.\n",
    "1. Parametric methods involve a two-step model-based approach:\n",
    "\n",
    "    1.1 Make an assumption about the functional form, or shape, of $f$ (OLS => i.e. $f$ is linear in $X$)\n",
    "$$ f(X)=β_0 +β_1X_1 +β_2X_2 +...+β_pX_p.$$\n",
    "    \n",
    "    1.2 Once a model is selected we need a procedure that uses the training data to _fit_ or train the model In the case of our linear model we need to estimate parameters $β_0$+$β_1$+$β_2$+...+$β_p$. This means that we need to find values of these parameters so that\n",
    "    \n",
    "$$Y ≈β_0 +β_1X_1 +β_2X_2 +...+β_pX_p$$\n",
    "\n",
    "where “≈” as “is approximately modeled as”.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](https://snag.gy/iVykz0.jpg)\n",
    "*Image from Python Machine Learning - Sebastian Raschka*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Leaning - OLS - Assesing the accuracy of the model\n",
    "* Let $\\hat Y_i$ = $\\hat β_0$ + $\\hat β_1x_i$ be the prediction for $Y$ based on the $i$th value of $X$. \n",
    "* Then $e_i$ = $y_i$ − $\\hat y_i$ represents the $i$th residual => this is the difference between the $i$th observed response value and the $i$th response value that is predicted by a  linear model\n",
    "* We define the residual sum of squares (RSS) as\n",
    "$$R S S = e^2_1 + e^2_2 + · · · + e^2_n$$\n",
    "\n",
    "* Residual Standard Error (RSE), associated with each observation is an error term $ε$, and is because the presence of the error terms that we are unable to predict $Y$ from $X$\n",
    "* RSE is an estimate of the standard deviation of $ε$, thus RSE is\n",
    "$$RSE = \\sqrt {\\frac{1} {n-2} RSS}$$\n",
    "\n",
    "Where \n",
    "$$R S S  = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2  $$\n",
    "\n",
    "\n",
    "* Mean Square Error (MSE) => average of the sum of the square errors\n",
    "$$ MSE =\\frac 1n\\sum_{i=1}^n (y_i - \\hat{y}_i)^2  $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Learning - the least squares loss function\n",
    "\n",
    "You've become familiar at this point with the least squares loss function. Vanilla regression minimizes the residual sum of squares (RSS) to fit the data:\n",
    "\n",
    "$$ \\text{minimize}\\; RSS = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^n \\left(y_i - \\left(\\beta_0 + \\sum_{j=1}^p\\beta_j x_j\\right)\\right)^2 $$\n",
    "\n",
    "Where our model predictions for $y$ are based on the sum of the $beta_0$ intercept and the products of $\\beta_i$ with $x_i$.\n",
    "\n",
    "[Loss Function] [1]\n",
    "[1]: https://davidrosenberg.github.io/ml2015/docs/3a.loss-functions.pdf \"Loss Function\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Leaning - Regularization\n",
    "* Rather than simply choosing the best fit, there is some penalty to avoid over-fitting.\n",
    "    * Model suffers from **overfitting** => high variance (too many parameters that create a complex model)\n",
    "    * Model suffers from **underfitting** => high bias (model is not complex enough to capture patterns on our train data, suffers from  low perfomance\n",
    "    * Here lies the bias-variance tradeoff, but we can tune the complexity of the model via **regularization**\n",
    "    * **Regularization** handles  **multicollinearity**, filters out noise from data, eventually will prevent overfitting \n",
    "    * **Regularization** the idea is to introduce additional information (bias) to penalize extreme parameter weights\n",
    "\n",
    "* Two issues: choosing the form of the regularization, and choosing the amount of regularization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Learning - Ridge regression $L_2$\n",
    "\n",
    "The Ridge regression adds an additional thing to the loss function: the sum of the squared (non-intercept!) $\\beta$ values:\n",
    "$$ \\text{minimize}\\; RSS+Ridge = \\sum_{i=1}^n \\left(y_i - \\left(\\beta_0 + \\sum_{j=1}^p\\beta_j x_j\\right)\\right)^2 + \\lambda_2\\sum_{j=1}^p \\beta_j^2$$\n",
    "\n",
    "What are these new components?\n",
    "\n",
    "$\\beta_j^2$ is the squared coefficient for variable $x_j$.\n",
    "\n",
    "$\\sum_{j=1}^n \\beta_ij^2$ is the sum of these squared coefficients for every variable we have in our model. This does **not** include the intercept $\\beta_0$.\n",
    "\n",
    "$\\lambda_2$ is a constant for the _strength_ of the regularization parameter. The higher this value, the greater the impact of this new component in the loss function. If this were zero, then we would revert back to just the least squares loss function. If this were, say, a billion, then the residual sum of squares component would have a much smaller effect on the loss/cost than the regularization term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#machine learning will help overcoming the two issues by finding the perfect value of lambda. \n",
    "#Lambda is a weight apply to the correction/penalty term of the Beta\n",
    "#Ridge assumes that x are related, shrink coefficients down, instead of deleting multicollinearity between Xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Learning - Lasso regression $L_1$\n",
    "\n",
    "The Lasso regression takes a different approach. Instead of adding the sum of _squared_ $\\beta$ coefficients to the RSS, it adds the sum of the _absolute value_ of the $\\beta$ coefficients:\n",
    "\n",
    "$$ \\text{minimize}\\; RSS + Lasso = \\sum_{i=1}^n \\left(y_i - \\left(\\beta_0 + \\sum_{j=1}^p\\beta_j x_j\\right)\\right)^2 + \\lambda_1\\sum_{j=1}^p |\\beta_j|$$\n",
    "\n",
    "$|\\beta_j|$ is the absolute value of the $\\beta$ coefficient for variable $x_j$\n",
    "\n",
    "$\\lambda_1$ is again the strength of the regularization penalty component in the loss function. In lasso the lambda is denoted with a 1, in ridge the lambda is denoted with a 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#But Lasso on the other side, does not reduce the value of correlated Xs, it just delete one to remove multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Learning - Elastic Net\n",
    "\n",
    "Elastic Net is a combination of both the Lasso and the Ridge regularizations. It adds both penalties to the loss function:\n",
    "\n",
    " $$ \\text{minimize}\\; RSS + Ridge + Lasso = \\sum_{i=1}^n \\left(y_i - \\left(\\beta_0 + \\sum_{j=1}^p\\beta_j x_j\\right)\\right)^2 + \\lambda_1\\sum_{j=1}^p |\\beta_j| + \\lambda_2\\sum_{j=1}^p \\beta_j^2$$\n",
    "\n",
    "In the elastic net, the effect of the Ridge vs. the Lasso is balanced by the two lambda parameters. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Elastic Net combines both methods\n",
    "# N.B: HERE WE ARE NOT TRYING TO EXPLAIN REG COEFFICIENTS, INSTEAD FINDING THE TECHNIQUE WITH THE SMALLEST MEAN SQUARE ERROR.\n",
    "#We compare OLS vs Lasso vs Ridge vs Elastic Net.. Which one is better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Learning - so when do you use each? what is the effect of regularization?\n",
    "\n",
    "When you have **multicollinearity** in the data.\n",
    "\n",
    "The Lasso and Elastic Net are also very useful for when you have redundant or unimportant variables. If you have 1000 variables in a dataset the Lasso can perform \"feature selection\" automatically for you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Learning - normalize predictors \n",
    "With the Lasso and Ridge it is neccessary to **standarized** the predictor columns before constructing models, even the dummy coded categorical variables. \n",
    "\n",
    "\n",
    "#### Why is normalization of predictors required?\n",
    "\n",
    "Recall the equations for the Ridge and Lasso penalties:\n",
    "\n",
    "$$ \\text{Ridge penalty}\\; = \\lambda_2\\sum_{j=1}^p \\beta_j^2$$\n",
    "\n",
    "$$ \\text{Lasso penalty}\\; = \\lambda_2\\sum_{j=1}^p |\\beta_j|$$\n",
    "\n",
    "**How are the $\\beta$ coefficients affected by the mean and variance of your variables?**\n",
    "\n",
    "If the mean and variance of your $x$ predictors are different, their respective $\\beta$ coefficients _scale with the mean and variance of the predictors **regardless of their explanatory power.**_\n",
    "\n",
    "This means that if one of your $x$ variables, for example the price of a home, will have a much smaller $\\beta$ value than say the number of bedrooms in a house – just because the scale of the two variables are so different.\n",
    "\n",
    "The Ridge and Lasso penalties are **agnostic** to the **mean** and **variance** of your predictors. All they \"care about\" are the values of the coefficients. If one of your coefficients is much larger than any of the others, it will dominate the effect of the penalty on your minimization!\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
