{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://images.efollett.com/htmlroot/images/templates/storeLogos/CA/864.gif\" style=\"float: right;\"> \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ECON628-01 \n",
    "### Lecture 1.2 - CART (Classification and Regression Trees) and Random Forest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Leaning - CART \n",
    "\n",
    "\n",
    "Classification and regression trees or CART models, also called **decision trees**:\n",
    "* Non-parametric supervised learning method used for classification and regression\n",
    "* Recursively partitioning the input space, and defining a local model in each resulting region of input space. \n",
    "* These methods involve stratifying or segmenting the predictor space into a number of simple regions. \n",
    "* To make a prediction for a given observation, the method typically use the mean or the mode of the training observations in the region to which it belongs. \n",
    "* The set of splitting rules used to segment the predictor space can be summarized in a tree, therefore the name of **decision tree** methods.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Leaning - CART\n",
    "* Recall that a **Linear Regression** is a global model => there is one simple predictive formula **holding** over the entire space.\n",
    "* **BUT** when the data has numerous features that interact in complicated **nonlinear** ways => creating a single global model can be remarkably challenging.\n",
    "* Alternative approach is to sub-divide or **partition** the space into smaller regions, and then partition the subdivision again **(recursive partitioning** process) until we get to chunks of the space that are easier to control so that we can ultimately fit simple models to them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Leaning - Tree Components\n",
    "\n",
    "![](https://snag.gy/wBTr86.jpg)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Leaning - Regression Tree - recursive partition\n",
    "* Each of the terminal nodes, or leaves, of the tree represents a cell of the partition, and has attached to it a simple model which applies in that **cell only**. \n",
    "* A point $x$ belongs to a leaf if $x$ falls in the corresponding cell of the partition.\n",
    "* To figure out which cell we are in, we start at the **root node** of the tree, and ask a sequence of questions about the features.\n",
    "* The interior nodes are labeled with questions, and the edges or branches between them labeled by the answers. Which question we ask next depends on the answers to previous questions. \n",
    "* The variables do not all have to be of the same type; some can be continuous, some can be discrete but ordered, some can be categorical, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tree PARTITIONS THE DATASET INTO SUBSET.\n",
    "# EXPLE IF GENDER IS MALE, DO THAT, IF FEMALE DO THIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Leaning - Regression Tree - example\n",
    "Predicting log of salary of a baseball player, based on # of years played in MLB and # hits made on previous year.\n",
    "\n",
    "![](https://snag.gy/dETZJ3.jpg)\n",
    "![](https://snag.gy/S0RaQK.jpg)\n",
    "*Image from An Introduction to Statistical Learning *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Machine Leaning - Regression Tree - example\n",
    "In the previous example we see 3 regions defined as:\n",
    "* $R_1$ = {$X$ | $Years$ $<$ 4.5}\n",
    "* $R_2$ = {$X$ | $Years$ $>$ = 4.5, $Hits$ $<$ 117.5} \n",
    "* $R_3$ = {$X$ | $Years$ $>$ = 4.5, $Hits$ $>$ = 117.5}\n",
    "\n",
    "![](https://snag.gy/Is4wZe.jpg)\n",
    "*Image from An Introduction to Statistical Learning *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Leaning - Regression Tree - cell models\n",
    "* The model in each cell is just a constant estimate of $Y$. \n",
    "* Meaning => suppose the points ($x_i$, $y_i$), ($x_2$, $y_2$),...($x_c$, $y_c$) are all the samples belonging to the leaf-node $l$. \n",
    "* Then, the model for $l$ is the the sample mean of the dependent variable in that cell:\n",
    "$$\\hat y = \\frac1 c \\sum_{i=1}^c y_i $$ \n",
    "\n",
    "* Once the local models are completely determined, we need to **find** a good tree (this means finding a good partitioning of the data)\n",
    "* In **regression trees** we do this by maximizing $I$ $[$ $C$ ; $Y$ $]$ where $Y$ is now the dependent variable, and \n",
    "$C$ is the variable saying which leaf of the tree we end up at.\n",
    "* But we can't do a direct maximization, so we need to do a **greedy search** => find the binary question that **maximizes** the information we get about $Y$, this will create the root node and 2 daughters nodes. Then at each daughter node we repeat this procedure asking which question would give us the maximum information about $Y$ , given where we already are in the tree => we repeat this **recursively**\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#INSTEAD OF MINIMIZING, LIKE WITH REGULARIZATION, WE MAXIMIZE BY LEAVE OR FOR EXPLE BY MALE OR FEMALE\n",
    "#AND IF FEMALE, CONSIDER NO DATA, BUT FOR MALE, IF INCOME IS <10000, THEN GIVE ME THE OUTCOME AVERAGE AGE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Machine Leaning - Regression Tree - cell models\n",
    "* To avoid end up putting every point in its own leaf-node, we need to use a **stopping criterion.**\n",
    "* This means stop growing the tree when further splits gives less than some minimal amount of extra information.\n",
    "* In doing this the sum of squared errors for a tree $T$ is:\n",
    "$$S = \\sum_{c \\in leaves (T)}  \\sum_{i \\in C} (y_i - m_c)^2  $$ \n",
    "* where the prediction for leave $c$ is: \n",
    "$$m_c = \\frac 1 n_c \\sum_{c \\in leaves C} y_i $$ \n",
    "* therefore we can rewrite $S$ as: \n",
    "$$S = \\sum_{c \\in leaves (T)} n_c V_C  $$ \n",
    "* where $V_c$ is the within leave variance of leaf $c$, and now we can make our splits to minimize $S$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Leaning - Tree pruning\n",
    "* In the minimization process we are more likely to create good predictions on the training set, but there is a **high** risk of **overfitting** the data => leads to a poor test performance.\n",
    "* To avoid this we need to prune our tree => select a subtree that leads to a lower error rate CV and more\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Leaning - Pros- Cons\n",
    "| **Pros**   |      **Cons**     | \n",
    "|:----------|:-------------:|\n",
    "| Simple to understand/interpret | Overfitting | \n",
    "| Little data preparation (normalization, create dummies) | Greedy algorithm |\n",
    "|Easily handle mixed discrete and continuous inputs|Unstable: small changes to the input data=>large effects on the structure of the tree|\n",
    "|Insensitive to monotone transformations of the input|Trees are high **variance** estimators (BECAUSE OF TOO MUCH SPECIFICATIONS OR CONDITIONS|\n",
    "|Perform automatic variable selection|\n",
    "|Relatively robust to outliers|\n",
    "|Robust - performs well when assumptions are violated|  \n",
    "|Performs well in large datasets |    \n",
    "|Extremely fast execution |    \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Leaning - Random Forest\n",
    "* One way to reduce variance of an estimate is to average together many estimates.\n",
    "* We can train $M$ different trees on different subsets of the data, **choosing randomly with replacement** and then compute the **ensemble:**\n",
    "$$ f(X)= \\sum_{m=1}^M \\frac 1M f_m(X)$$\n",
    "\n",
    "where $f_m$ is the $m$’th tree. \n",
    "* This technique is called **bagging** => “bootstrap aggregating”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#THIS METHOD HELPS REDUCING THE VARIANCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Machine Leaning - Random Forest\n",
    "* Re-running the same learning algorithm on different subsets of the data can result in highly correlated predictors.\n",
    "* This limits the amount of variance reduction that is possible. \n",
    "* Random forests tries to decorrelate the base learners by learning trees based on a **randomly** chosen subset of input variables, as well as a **randomly** chosen subset of data cases.\n",
    "* Random forests often have very good predictive accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
